---
layout: about
title: about üçä
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: clem_funpic.jpeg
  # address: >
  #   <p>555 your office number</p>
  #   <p>123 your address street</p>
  #   <p>Your City, State 12345</p>
social: true  # includes social icons at the bottom of the page
news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
---
I am passionate and fascinated by the complex, flexible and optimized functioning of the brain that is yet left to be understood and characterized. I am particularly interested in studying the neural computational theories at the basis of information processing, learning and memory formation in neuronal networks. I am interested in answering questions such as:

<ul>
    <li>How is information from the environment represented by ensembles of neurons?</li>
    <li> How do the neural networks evolve with learning?</li>
    <li> How does the brain create, store, and update memories for places and events?</li>
</ul> 
I have made advancements in answering these questions during my Ph.D. with [Andrew SAXE](https://www.sainsburywellcome.org/web/groups/saxe-lab) and [Caswell Barry](https://barry-lab.com), working at the intersection between theoretical neuroscience and theoretical machine learning. It is my conviction that working on both artificial and biological
neural networks in parallel will enable us to move forward faster in the general understanding of their mechanisms. My research seeks to develop mathematical toolkits suitable for analyzing and describing learning mechanisms. In particular, I focus on the theory of deep learning, a class of artificial neural network models that take inspiration from the brain. In our recently published paper, we derive exact solutions to the dynamics of learning with rich prior knowledge in deep linear networks by generalizing Fukumizu's matrix Riccati solution. We discuss its implication in continual learning, reversal learning, and learning of structured knowledge settings.  I have also probed flexible learning behaviors in large deep network models
as well as high-dimensional neural data recorded from rodents as they learn. By combining theory, deep learning engineering, and inspiration from biological intelligence, I hope to improve deep learning systems' ability to continually learn while deployed, flexibly revising and adapting their knowledge to new users and contexts.

***